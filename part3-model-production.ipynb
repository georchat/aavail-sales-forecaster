{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./documentation/images/ibm-logo.png\" alt=\"ibm-logo\" align=\"center\" style=\"width: 200px;\"/>\n",
    "\n",
    "**AI ENTERPRISE WORKFLOW CERTIFICATION**\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Capstone Project - Model Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "1. Build a draft version of an API with train, predict, and logfile endpoints.\n",
    "2. Using Docker, bundle your API, model, and unit tests.\n",
    "3. Using test-driven development iterate on your API in a way that anticipates scale, load, and drift.\n",
    "4. Create a post-production analysis script that investigates the relationship between model performance and the business metric.\n",
    "5. Articulate your summarized findings in a final report.\n",
    "\n",
    "At a higher level you are being asked to:\n",
    "\n",
    "1. Ready your model for deployment\n",
    "2. Query your API with new data and test your monitoring tools\n",
    "3. Compare your results to the gold standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create a flask API\n",
    "\n",
    "The API performs the train, predict and logfile tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import argparse\n",
    "from flask import Flask, jsonify, request\n",
    "from flask import render_template\n",
    "import joblib\n",
    "import socket\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "## import model specific functions and variables\n",
    "from modelling import *\n",
    "from logger import *\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    html = \"<h3>Hello {name}!</h3>\" \\\n",
    "           \"<b>Hostname:</b> {hostname}<br/>\"\n",
    "    return html.format(name=os.getenv(\"NAME\", \"world\"), hostname=socket.gethostname())\n",
    "\n",
    "@app.route('/predict', methods=['GET','POST'])\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    basic predict function for the API\n",
    "    \"\"\"\n",
    "    \n",
    "    ## input checking\n",
    "    if not request.json:\n",
    "        print(\"ERROR: API (predict): did not receive request data\")\n",
    "        return jsonify([])\n",
    "    \n",
    "    if 'country' not in request.json:\n",
    "        print(\"ERROR API (predict): received request, but no 'country' found within\")\n",
    "        return jsonify(False)\n",
    "        \n",
    "    if 'year' not in request.json:\n",
    "        print(\"ERROR API (predict): received request, but no 'year' found within\")\n",
    "        return jsonify(False)\n",
    "        \n",
    "    if 'month' not in request.json:\n",
    "        print(\"ERROR API (predict): received request, but no 'month' found within\")\n",
    "        return jsonify(False)\n",
    "        \n",
    "    if 'day' not in request.json:\n",
    "        print(\"ERROR API (predict): received request, but no 'day' found within\")\n",
    "        return jsonify(False)\n",
    "    \n",
    "    if 'dev' not in request.json:\n",
    "        print(\"ERROR API (predict): received request, but no 'dev' found within\")\n",
    "        return jsonify([])\n",
    "    \n",
    "    if 'verbose' not in request.json:\n",
    "        print(\"WARNING API (predict): received request, but no 'verbose' found within\")\n",
    "        verbose = 'True'\n",
    "    else:\n",
    "        verbose = request.json['verbose']\n",
    "        \n",
    "    ## predict\n",
    "    _result = result = model_predict(year=request.json['year'],\n",
    "                                     month=request.json['month'],\n",
    "                                     day=request.json['day'],\n",
    "                                     country=request.json['country'],\n",
    "                                     dev=request.json['dev']==\"True\",\n",
    "                                     verbose=verbose==\"True\")\n",
    "    \n",
    "    result = {}\n",
    "    ## convert numpy objects so ensure they are serializable\n",
    "    for key,item in _result.items():\n",
    "        if isinstance(item,np.ndarray):\n",
    "            result[key] = item.tolist()\n",
    "        else:\n",
    "            result[key] = item\n",
    "\n",
    "    return(jsonify(result))\n",
    "\n",
    "@app.route('/train', methods=['GET','POST'])\n",
    "def train():\n",
    "    \"\"\"\n",
    "    basic train function for the API\n",
    "\n",
    "    the 'dev' give you the ability to toggle between a DEV version and a PROD verion of training\n",
    "    \"\"\"\n",
    "\n",
    "    if not request.json:\n",
    "        print(\"ERROR: API (train): did not receive request data\")\n",
    "        return jsonify(False)\n",
    "\n",
    "    if 'dev' not in request.json:\n",
    "        print(\"ERROR API (train): received request, but no 'dev' found within\")\n",
    "        return jsonify(False)\n",
    "    \n",
    "    if 'verbose' not in request.json:\n",
    "        print(\"WARNING API (predict): received request, but no 'verbose' found within\")\n",
    "        verbose = 'True'\n",
    "    else:\n",
    "        verbose = request.json['verbose']\n",
    "\n",
    "    print(\"... training model\")\n",
    "    model = model_train(dev=request.json['dev']==\"True\", verbose=verbose==\"True\")\n",
    "    print(\"... training complete\")\n",
    "\n",
    "    return(jsonify(True))\n",
    "\n",
    "@app.route('/logging', methods=['GET','POST'])\n",
    "def load_logs():\n",
    "    \"\"\"\n",
    "    basic logging function for the API\n",
    "    \"\"\"\n",
    "\n",
    "    if not request.json:\n",
    "        print(\"ERROR: API (train): did not receive request data\")\n",
    "        return jsonify(False)\n",
    "\n",
    "    if 'env' not in request.json:\n",
    "        print(\"ERROR API (log): received request, but no 'env' found within\")\n",
    "        return jsonify(False)\n",
    "        \n",
    "    if 'type' not in request.json:\n",
    "        print(\"ERROR API (log): received request, but no 'type' found within\")\n",
    "        return jsonify(False)\n",
    "        \n",
    "    if 'month' not in request.json:\n",
    "        print(\"ERROR API (log): received request, but no 'month' found within\")\n",
    "        return jsonify(False)\n",
    "        \n",
    "    if 'year' not in request.json:\n",
    "        print(\"ERROR API (log): received request, but no 'year' found within\")\n",
    "        return jsonify(False)\n",
    "    \n",
    "    print(\"... fetching logfile\")\n",
    "    logfile = log_load(env=request.json['env'],\n",
    "                       tag=request.json['type'],\n",
    "                       year=request.json['year'],\n",
    "                       month=request.json['month'])\n",
    "    \n",
    "    result = {}\n",
    "    result[\"logfile\"]=logfile\n",
    "    return(jsonify(result))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ## parse arguments for debug mode\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"-d\", \"--debug\", action=\"store_true\", help=\"debug flask\")\n",
    "    args = vars(ap.parse_args())\n",
    "\n",
    "    if args[\"debug\"]:\n",
    "        app.run(debug=True, port=8080)\n",
    "    else:\n",
    "        app.run(host='0.0.0.0', threaded=True ,port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Test the Flask API**\n",
    "\n",
    "From the project directory I started the app:\n",
    "\n",
    "```bash\n",
    "$ python app.py\n",
    "```\n",
    "\n",
    "Then went to [http://localhost:8080/](http://localhost:8080/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y_pred': [183770.3050000001]}\n"
     ]
    }
   ],
   "source": [
    "## API predict\n",
    "import requests\n",
    "from ast import literal_eval\n",
    "\n",
    "query = {\"year\":\"2018\",\"month\":\"1\",\"day\":\"5\",\"country\":\"total\",\"dev\":\"True\",\"verbose\":\"True\"}\n",
    "port = 8080\n",
    "r = requests.post('http://localhost:{}/predict'.format(port),json=query)\n",
    "response = literal_eval(r.text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## API train\n",
    "query = {\"dev\":\"True\",\"verbose\":\"True\"}\n",
    "port = 8080\n",
    "r = requests.post('http://localhost:{}/train'.format(port),json=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logfile': 'test-train-2020-5.log'}\n"
     ]
    }
   ],
   "source": [
    "## API logging\n",
    "query = {\"env\":\"test\",\"type\":\"train\",\"year\":\"2020\",\"month\":\"5\"}\n",
    "port = 8080\n",
    "r = requests.post('http://localhost:{}/logging'.format(port),json=query)\n",
    "response = literal_eval(r.text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "I stopped the server.  We will relaunch it in a few moments from within Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create Unit Tests\n",
    "\n",
    "The unit tests are usually organized as a suite and return objective evidence, in the form of a boolean value, which is a key element that enables workflow automation. The boolean value indicates whether or not each and every part of the software that was tested performed as expected. Much like data ingestion, the idea is to have the necessary components of a task bundled under a single script. In this case it will be called `run-tests.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./unittests/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./unittests/__init__.py\n",
    "\n",
    "import unittest\n",
    "import getopt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## parse inputs\n",
    "try:\n",
    "    optlist, args = getopt.getopt(sys.argv[1:],'v')\n",
    "except getopt.GetoptError:\n",
    "    print(getopt.GetoptError)\n",
    "    print(sys.argv[0] + \"-v\")\n",
    "    print(\"... the verbose flag (-v) may be used\")\n",
    "    sys.exit()\n",
    "\n",
    "VERBOSE = False\n",
    "RUNALL = False\n",
    "\n",
    "sys.path.append(os.path.realpath(os.path.dirname(__file__)))\n",
    "\n",
    "for o, a in optlist:\n",
    "    if o == '-v':\n",
    "        VERBOSE = True\n",
    "\n",
    "## api tests\n",
    "from ApiTests import *\n",
    "ApiTestSuite = unittest.TestLoader().loadTestsFromTestCase(ApiTest)\n",
    "\n",
    "## model tests\n",
    "from ModelTests import *\n",
    "ModelTestSuite = unittest.TestLoader().loadTestsFromTestCase(ModelTest)\n",
    "\n",
    "## logger tests\n",
    "from LoggerTests import *\n",
    "LoggerTestSuite = unittest.TestLoader().loadTestsFromTestCase(LoggerTest)\n",
    "\n",
    "MainSuite = unittest.TestSuite([ApiTestSuite,ModelTestSuite,LoggerTestSuite])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Test-Driven Development\n",
    "\n",
    "Create Unit Tests for Model, Logger and the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./unittests/ModelTests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./unittests/ModelTests.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "model tests\n",
    "\"\"\"\n",
    "\n",
    "import unittest\n",
    "from modelling import *\n",
    "\n",
    "class ModelTest(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    test the essential functionality\n",
    "    \"\"\"\n",
    "    \n",
    "    def test_01_train(self):\n",
    "        \"\"\"\n",
    "        test the train functionality\n",
    "        \"\"\"\n",
    "    \n",
    "        ## train the model\n",
    "        model_train(verbose=False)\n",
    "        \n",
    "        prefix = \"test\" if DEV else \"prod\"\n",
    "        models = [f for f in os.listdir(MODEL_DIR) if re.search(prefix,f)]\n",
    "        self.assertEqual(len(models),11)\n",
    "        \n",
    "    def test_02_load(self):\n",
    "        \"\"\"\n",
    "        test the train functionality\n",
    "        \"\"\"\n",
    "        \n",
    "        ## load the model\n",
    "        models = model_load(verbose=False)\n",
    "        \n",
    "        for tag, model in models.items():\n",
    "            self.assertTrue(\"predict\" in dir(model))\n",
    "            self.assertTrue(\"fit\" in dir(model))\n",
    "        \n",
    "    def test_03_predict(self):\n",
    "        \"\"\"\n",
    "        test the predict function input\n",
    "        \"\"\"\n",
    "    \n",
    "        ## query inputs\n",
    "        query = \"2018\", \"1\", \"5\", \"total\"\n",
    "        \n",
    "        ## load model first\n",
    "        result = model_predict(year=query[0], month=query[1], day=query[2], country=query[3], verbose=False)\n",
    "        y_pred = result[\"y_pred\"]\n",
    "        self.assertTrue(y_pred.dtype==np.float64)\n",
    "            \n",
    "    def test_04_predict(self):\n",
    "        \"\"\"\n",
    "        test the predict function accuracy\n",
    "        \"\"\"\n",
    "    \n",
    "        ## example predict\n",
    "        example_queries = [(\"2018\", \"1\", \"5\", \"total\"),\n",
    "                           (\"2019\", \"2\", \"5\", \"eire\"),\n",
    "                           (\"2018\", \"12\", \"5\", \"france\")]\n",
    "        \n",
    "        for query in example_queries:\n",
    "            result = model_predict(year=query[0], month=query[1], day=query[2], country=query[3], verbose=False)\n",
    "            y_pred = result[\"y_pred\"]\n",
    "            self.assertTrue(y_pred.dtype==np.float64)\n",
    "            \n",
    "## run the tests\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 101.067s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run ./unittests/ModelTests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./unittests/LoggerTests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./unittests/LoggerTests.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "logger tests\n",
    "\"\"\"\n",
    "\n",
    "import unittest\n",
    "## import model specific functions and variables\n",
    "from logger import *\n",
    "\n",
    "class LoggerTest(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    test the essential log functionality\n",
    "    \"\"\"\n",
    "        \n",
    "    def test_01_train(self):\n",
    "        \"\"\"\n",
    "        test the train functionality\n",
    "        \"\"\"\n",
    "\n",
    "        ## train logfile\n",
    "        today = date.today()\n",
    "        logfile = \"{}-train-{}-{}.log\".format(\"test\",today.year,today.month)\n",
    "        log_path = os.path.join(LOG_DIR, logfile)\n",
    "        \n",
    "        self.assertTrue(os.path.exists(log_path))\n",
    "\n",
    "    def test_02_predict(self):\n",
    "        \"\"\"\n",
    "        test the predict functionality\n",
    "        \"\"\"\n",
    "        \n",
    "        ## train logfile\n",
    "        today = date.today()\n",
    "        logfile = \"{}-predict-{}-{}.log\".format(\"test\",today.year,today.month)\n",
    "        log_path = os.path.join(LOG_DIR, logfile)\n",
    "        \n",
    "        self.assertTrue(os.path.exists(log_path))\n",
    "\n",
    "    def test_03_load(self):\n",
    "        \"\"\"\n",
    "        test the load functionality\n",
    "        \"\"\"\n",
    "\n",
    "        ## load model first\n",
    "        logfile = log_load(env=\"test\",tag=\"train\",year=2020,month=5, verbose=False)\n",
    "        logpath = os.path.join(LOG_DIR, logfile)\n",
    "        with open(logpath, \"r\") as log:\n",
    "            text = log.read()\n",
    "        self.assertTrue(len(text.split(\"\\n\"))>2)\n",
    "\n",
    "        \n",
    "### Run the tests\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.006s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run ./unittests/LoggerTests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./unittests/ApiTests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./unittests/ApiTests.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "api tests\n",
    "\n",
    "these tests use the requests package however similar requests can be made with curl\n",
    "\n",
    "e.g.\n",
    "data = '{\"key\":\"value\"}'\n",
    "curl -X POST -H \"Content-Type: application/json\" -d \"%s\" http://localhost:8080/predict'%(data)\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import unittest\n",
    "import requests\n",
    "import re\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "port = 8080\n",
    "\n",
    "try:\n",
    "    requests.post('http://localhost:{}/predict'.format(port))\n",
    "    server_available = True\n",
    "except:\n",
    "    server_available = False\n",
    "    \n",
    "## test class for the main window function\n",
    "class ApiTest(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    test the essential functionality\n",
    "    \"\"\"\n",
    "    \n",
    "    @unittest.skipUnless(server_available,\"local server is not running\")\n",
    "    def test_predict(self):\n",
    "        \"\"\"\n",
    "        test the predict functionality\n",
    "        \"\"\"\n",
    "        \n",
    "        query = {\"year\":\"2018\",\"month\":\"1\",\"day\":\"5\",\"country\":\"total\",\"dev\":\"True\",\"verbose\":\"True\"}\n",
    "        r = requests.post('http://localhost:{}/predict'.format(port),json=query)\n",
    "        response = literal_eval(r.text)\n",
    "        self.assertTrue(isinstance(response[\"y_pred\"][0], float))\n",
    "\n",
    "    @unittest.skipUnless(server_available,\"local server is not running\")\n",
    "    def test_train(self):\n",
    "        \"\"\"\n",
    "        test the train functionality\n",
    "        \"\"\"\n",
    "      \n",
    "        query = {\"dev\":\"True\",\"verbose\":\"False\"}\n",
    "        r = requests.post('http://localhost:{}/train'.format(port),json=query)\n",
    "        train_complete = re.sub(\"\\W+\",\"\",r.text)\n",
    "        self.assertEqual(train_complete,'true')\n",
    "        \n",
    "    @unittest.skipUnless(server_available,\"local server is not running\")\n",
    "    def test_logging(self):\n",
    "        \"\"\"\n",
    "        test the logging functionality\n",
    "        \"\"\"\n",
    "        \n",
    "        query = {\"env\":\"test\",\"type\":\"train\",\"year\":\"2020\",\"month\":\"5\"}\n",
    "        r = requests.post('http://localhost:{}/logging'.format(port),json=query)\n",
    "        response = literal_eval(r.text)\n",
    "        self.assertEqual(response.get(\"logfile\"),'test-train-2020-5.log')\n",
    "\n",
    "### Run the tests\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 95.657s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run ./unittests/ApiTests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run-tests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run-tests.py\n",
    "#!/usr/bin/python \n",
    "\n",
    "import sys\n",
    "import unittest\n",
    "\n",
    "from unittests import *\n",
    "unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Run Unit Tests with a single script**\n",
    "\n",
    "```bash\n",
    "    ~$ python run-tests.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Docker Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Create Docker File**\n",
    "\n",
    "Before we build the DockerFile first we need to create a requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "cython\n",
    "numpy\n",
    "flask\n",
    "pandas\n",
    "scikit-learn\n",
    "matplotlib\n",
    "IPython\n",
    "seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "# Use an official Python runtime as a parent image\n",
    "FROM python:3.7.5-stretch\n",
    "\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "python3-dev \\\n",
    "build-essential    \n",
    "        \n",
    "# Set the working directory to /app\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "ADD . /app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Make port 80 available to the world outside this container\n",
    "EXPOSE 80\n",
    "\n",
    "# Define environment variable\n",
    "ENV NAME World\n",
    "\n",
    "# Run app.py when the container launches\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Build the Docker image and run it**\n",
    "\n",
    "Step one: build the image (from the directory that was created with this notebook)\n",
    " \n",
    "```bash\n",
    "    ~$ docker build -t capstone-ml-app .\n",
    "```\n",
    "\n",
    "Check that the image is there.\n",
    "\n",
    "```bash\n",
    "    ~$ docker image ls\n",
    "```\n",
    "\n",
    "You may notice images that you no longer use.  You may delete them with\n",
    "\n",
    "```bash\n",
    "    ~$ docker image rm IMAGE_ID_OR_NAME\n",
    "```\n",
    "\n",
    "Run the container\n",
    "\n",
    "```bash\n",
    "docker run -p 4000:8080 capstone-ml-app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Test the running app**\n",
    "\n",
    "First go to [http://localhost:4000/](http://localhost:4000/) to ensure the app is running and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y_pred': [183770.3050000001]}\n"
     ]
    }
   ],
   "source": [
    "## API predict\n",
    "\n",
    "query = {\"year\":\"2018\",\"month\":\"1\",\"day\":\"5\",\"country\":\"total\",\"dev\":\"True\",\"verbose\":\"True\"}\n",
    "port = 4000\n",
    "r = requests.post('http://localhost:{}/predict'.format(port),json=query)\n",
    "response = literal_eval(r.text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logfile': 'test-train-2020-5.log'}\n"
     ]
    }
   ],
   "source": [
    "## API logging\n",
    "\n",
    "query = {\"env\":\"test\",\"type\":\"train\",\"year\":\"2020\",\"month\":\"5\"}\n",
    "port = 4000\n",
    "r = requests.post('http://localhost:{}/logging'.format(port),json=query)\n",
    "response = literal_eval(r.text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Post Production Analysis\n",
    "### Perfomance Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here we have a function to simulate new samples using the bootstrap, which as a reminder is sampling with replacement. We measure the accuracy of the model and see how it changes when more data are accounted in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting monitoring.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile monitoring.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_engineering import engineer_features\n",
    "from modelling import _model_train, model_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy.stats import wasserstein_distance\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## switch to production\n",
    "DEV = False\n",
    "\n",
    "def simulate_samples(n_samples, X, y, dates):\n",
    "    \"\"\"\n",
    "    simulate new samples (via bootstrap)\n",
    "    \"\"\"\n",
    "    \n",
    "    indices = np.arange(y.size)\n",
    "    new_indices = np.random.choice(indices, n_samples, replace=True)\n",
    "    \n",
    "    X_new = X[new_indices,:]\n",
    "    y_new = y[new_indices]\n",
    "    dates_new = dates[new_indices]\n",
    "    return X_new, y_new, dates_new\n",
    "\n",
    "\n",
    "def model_monitor(country=\"total\", dev=DEV, training=True):\n",
    "    \"\"\"\n",
    "    performance monitoring\n",
    "    \"\"\"\n",
    "    print(\"Monitor Model\")\n",
    "    \n",
    "    ## import data\n",
    "    datasets = engineer_features(training=training, dev=dev)\n",
    "    X, y, dates, labels = datasets[country]\n",
    "    dates = pd.to_datetime(dates)\n",
    "    print(X.shape)\n",
    "    \n",
    "    ## train the model\n",
    "    if training:\n",
    "        _model_train(X, y, labels, tag=country, dev=dev)\n",
    "    \n",
    "    ## monitor RMSE\n",
    "    samples = [10, 20, 30, 50, 60]\n",
    "\n",
    "    for n in samples:\n",
    "        X_new, y_new, dates_new = simulate_samples(n, X, y, dates)\n",
    "        queries = [(str(d.year), str(d.month), str(d.day), country) for d in dates_new]\n",
    "        y_pred = [model_predict(year=query[0], month=query[1], day=query[2], country=query[3],verbose=False, dev=dev)[\"y_pred\"][0].round(2) for query in queries]\n",
    "        rmse = np.sqrt(mean_squared_error(y_new.tolist(),y_pred))\n",
    "        print(\"sample size: {}, RSME: {}\".format(n, rmse.round(2)))\n",
    "        \n",
    "    ## monitor performance\n",
    "    ## scaling\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    samples = [25, 50, 75, 90]\n",
    "\n",
    "    clf_y = EllipticEnvelope(random_state=0,contamination=0.01)\n",
    "    clf_X = EllipticEnvelope(random_state=0,contamination=0.01)\n",
    "\n",
    "    clf_X.fit(X)\n",
    "    clf_y.fit(y.reshape(y.size,1))\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    for n in samples:\n",
    "        X_new, y_new, dates_new = simulate_samples(n,X,y, dates)\n",
    "        results[\"sample_size\"].append(n)\n",
    "        results['wasserstein_X'].append(np.round(wasserstein_distance(X.flatten(),X_new.flatten()),2))\n",
    "        results['wasserstein_y'].append(np.round(wasserstein_distance(y,y_new),2))\n",
    "        test1 = clf_X.predict(X_new)\n",
    "        test2 = clf_y.predict(y_new.reshape(y_new.size,1))\n",
    "        results[\"outlier_percent_X\"].append(np.round(1.0 - (test1[test1==1].size / test1.size),2))\n",
    "        results[\"outlier_percent_y\"].append(np.round(1.0 - (test2[test2==1].size / test2.size),2))\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    run_start = time.time()\n",
    "  \n",
    "    ## monitor model\n",
    "    result = model_monitor(dev=DEV)\n",
    "    \n",
    "    print(result)\n",
    "    \n",
    "    m, s = divmod(time.time()-run_start,60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print(\"...running time:\", \"%d:%02d:%02d\"%(h, m, s))\n",
    "    \n",
    "    print(\"done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitor Model\n",
      "Ingesting Data\n",
      "...loading timeseries data from files\n",
      "Creating Feature Matrix\n",
      "Engineering Features and Target\n",
      "(98, 9)\n",
      "...training model: 4/4\n",
      "...best model:Random Forest\n",
      "...updating train log\n",
      "sample size: 10, RSME: 5982.89\n",
      "sample size: 20, RSME: 4945.57\n",
      "sample size: 30, RSME: 6439.1\n",
      "sample size: 50, RSME: 8671.57\n",
      "sample size: 60, RSME: 6158.0\n",
      "   sample_size  wasserstein_X  wasserstein_y  outlier_percent_X  \\\n",
      "0           25           0.14       13483.18               0.00   \n",
      "1           50           0.13        7720.49               0.02   \n",
      "2           75           0.12        5561.39               0.03   \n",
      "3           90           0.20       12258.64               0.03   \n",
      "\n",
      "   outlier_percent_y  \n",
      "0               0.00  \n",
      "1               0.02  \n",
      "2               0.00  \n",
      "3               0.00  \n",
      "...running time: 0:02:02\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "%run monitoring.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitor Model\n",
      "Ingesting Data\n",
      "...loading timeseries data from files\n",
      "Creating Feature Matrix\n",
      "Engineering Features and Target\n",
      "(98, 9)\n",
      "...training model: 4/4\n",
      "...best model:Random Forest\n",
      "...updating train log\n",
      "sample size: 10, RSME: 3869.4\n",
      "sample size: 20, RSME: 10556.31\n",
      "sample size: 30, RSME: 8421.46\n",
      "sample size: 50, RSME: 4625.63\n",
      "sample size: 60, RSME: 8074.12\n"
     ]
    }
   ],
   "source": [
    "from monitoring import model_monitor\n",
    "\n",
    "results_df = model_monitor(dev=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here I simulated a bunch of data with different sizes. The size ranges from 10 to 90 samples. We see that the RMSE score varies with regard to the size of the samples. This implies that if we were monitoring based on RMSE score along we would potentially need big samples of data to make conclusions. We were only able to calculate the RMSE scores because we know the true predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow0_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 100.0%, transparent 100.0%);\n",
       "        }    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow0_col2 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 100.0%, transparent 100.0%);\n",
       "        }    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow0_col3 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "        }    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow0_col4 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, red 50.0%, red 70.0%, transparent 70.0%);\n",
       "        }    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow1_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 66.0%, transparent 66.0%);\n",
       "        }    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow1_col2 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 82.6%, transparent 82.6%);\n",
       "        }    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow1_col3 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "        }    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow1_col4 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "        }    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow2_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 70.0%, transparent 70.0%);\n",
       "        }    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow2_col2 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 94.0%, transparent 94.0%);\n",
       "        }    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow2_col3 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, red 50.0%, red 55.0%, transparent 55.0%);\n",
       "        }    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow2_col4 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "        }    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow3_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 64.0%, transparent 64.0%);\n",
       "        }    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow3_col2 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 74.5%, transparent 74.5%);\n",
       "        }    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow3_col3 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, red 50.0%, red 55.0%, transparent 55.0%);\n",
       "        }    #T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow3_col4 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "        }</style><table id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0d\" ><caption>Performance Monitoring</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >sample_size</th>        <th class=\"col_heading level0 col1\" >wasserstein_X</th>        <th class=\"col_heading level0 col2\" >wasserstein_y</th>        <th class=\"col_heading level0 col3\" >outlier_percent_X</th>        <th class=\"col_heading level0 col4\" >outlier_percent_y</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow0_col0\" class=\"data row0 col0\" >25</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow0_col1\" class=\"data row0 col1\" >0.250000</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow0_col2\" class=\"data row0 col2\" >12223.910000</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow0_col4\" class=\"data row0 col4\" >0.040000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow1_col0\" class=\"data row1 col0\" >50</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow1_col1\" class=\"data row1 col1\" >0.080000</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow1_col2\" class=\"data row1 col2\" >7960.800000</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow1_col4\" class=\"data row1 col4\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow2_col0\" class=\"data row2 col0\" >75</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow2_col1\" class=\"data row2 col1\" >0.100000</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow2_col2\" class=\"data row2 col2\" >10751.800000</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow2_col3\" class=\"data row2 col3\" >0.010000</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow3_col0\" class=\"data row3 col0\" >90</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow3_col1\" class=\"data row3 col1\" >0.070000</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow3_col2\" class=\"data row3 col2\" >5991.430000</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow3_col3\" class=\"data row3 col3\" >0.010000</td>\n",
       "                        <td id=\"T_0e1863a2_929f_11ea_9944_a08cfd2abf0drow3_col4\" class=\"data row3 col4\" >0.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x152a0185588>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(results_df\n",
    " .style\n",
    " .hide_index()\n",
    " .bar(color='lightblue', vmin=0, subset=['wasserstein_X'], align='zero')\n",
    " .bar(color='lightblue', vmin=0, subset=['wasserstein_y'], align='zero')\n",
    " .bar(color='red', vmin=0, vmax=0.1,subset=['outlier_percent_X'], align='zero')\n",
    " .bar(color='red', vmin=0, vmax=0.1,subset=['outlier_percent_y'], align='zero')\n",
    " .set_caption('Performance Monitoring'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here I am testing for outliers, using the `EllipticEnvelop`, a variance based method. It is important to note that we are looking for outliers not distributional changes, which can be related, but they do not necessarily coincide. To specifically look for distributional changes I also use the `Wasserstein` metric. I ran both of these checks on the features and on the targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In a typical deployment scenario we would not have access to the true labels so we would not be able to calculate the wasserstein distance between known and predicted targets (shown in the 3rd column). We would however be able to check for outliers on the targets. We see that the distance metrics do a very good job detecting the drift in target distribution, the test that is the most important is of course most important at the level of X, because these data will be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# to convert the jupyter to slides presentation run in the command window the following\n",
    "# jupyter nbconvert --to slides --TemplateExporter.exclude_input=True part3-model-production.ipynb"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
